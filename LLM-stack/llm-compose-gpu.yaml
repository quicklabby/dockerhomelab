services:

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    runtime: nvidia
    restart: unless-stopped
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ./ollama:/root/.ollama
    ports:
      - 11434:11434
    networks: 
      ai-local:
        ipv4_address: 172.30.9.2

  translate:
    container_name: whisper-libretranslate
    image: libretranslate/libretranslate:latest-cuda
    restart: unless-stopped
    volumes:
      - ./whishper_libretranslate/libretranslate/data:/home/libretranslate/.local/share
      - ./whishper_libretranslate/libretranslate/cache:/home/libretranslate/.local/cache
    tty: true
    environment:
      LT_DISABLE_WEB_UI: True
      LT_LOAD_ONLY: ${LT_LOAD_ONLY:-en,fr,it}
      LT_UPDATE_MODELS: True
    expose:
      - 5000
    networks:
      ai-local:
        aliases:
          - translate
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

  whishper:
    pull_policy: always
    image: pluja/whishper:${WHISHPER_VERSION:-latest-gpu}
    volumes:
      - ./whishper_data/uploads:/app/uploads
      - ./whishper_data/logs:/var/log/whishper
    container_name: whishper
    restart: unless-stopped
    ports:
      - 8082:80
    depends_on:
      - translate
    environment:
      PUBLIC_INTERNAL_API_HOST: "http://127.0.0.1:80"
      PUBLIC_TRANSLATION_API_HOST: ""
      PUBLIC_API_HOST: ${WHISHPER_HOST:-}
      PUBLIC_WHISHPER_PROFILE: gpu
      WHISPER_MODELS_DIR: /app/models
      UPLOAD_DIR: /app/uploads
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
    networks:
      ai-local:

  wyoming-piper:  
    image: slackr31337/wyoming-piper-gpu:latest  
    container_name: wyoming-piper
    environment:  
      - PIPER_VOICE=en_US-amy-medium
    ports:  
      - 10200:10200
    volumes:
      - ./piper-models/en:/models
    restart: unless-stopped
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      ai-local:

  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest
    container_name: faster-whisper
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - WHISPER_MODEL=tiny-int8
      - WHISPER_BEAM=1 #optional
      - WHISPER_LANG=en #optional
    ports:
      - 10300:10300
    volumes:
      - ./wyoming-whisper:/models
    networks:
      ai-local:

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: anythingllm
    ports:
      - "3001:3001"
    cap_add:
      - SYS_ADMIN
    environment:
      - STORAGE_DIR=/app/server/storage     
      - OLLAMA_BASE_PATH=http://172.30.9.2:11434
    volumes:
      - ./anythingllm:/app/server/storage
    restart: unless-stopped
    networks: 
      ai-local:
        ipv4_address: 172.30.9.3


  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    ports:
      - "3002:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
      - ./searxng/searxng-data:/var/cache/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/ #add your hostname if you have one
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"
    networks:
      ai-local:
        ipv4_address: 172.30.9.4

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --cleanup --interval 86400


networks:
  ai-local:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.9.0/29

